{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dsl.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fGaw2e32Ab3u"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"markdown","source":["## Neural Functions"],"metadata":{"id":"GnjvgvhfCRlx"}},{"cell_type":"code","source":["def init_neural_function(input_type, output_type, input_size, output_size, num_units, device=\"cpu\"):\n","    if (input_type, output_type) == (\"list\", \"list\"):\n","        return ListToListModule(input_size, output_size, num_units, device=device)\n","    elif (input_type, output_type) == (\"list\", \"atom\"):\n","        return ListToAtomModule(input_size, output_size, num_units, device=device)\n","    elif (input_type, output_type) == (\"atom\", \"atom\"):\n","        return AtomToAtomModule(input_size, output_size, num_units, device=device)\n","    else:\n","        raise NotImplementedError\n","\n","\n","class HeuristicNeuralFunction:\n","\n","    def __init__(self, input_type, output_type, input_size, output_size, num_units, name, device):\n","        self.input_type = input_type\n","        self.output_type = output_type\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.num_units = num_units\n","        self.name = name\n","        self.device = device\n","        \n","        self.init_model()\n","\n","    def init_model(self):\n","        raise NotImplementedError\n","\n","class ListToListModule(HeuristicNeuralFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        super().__init__(\"list\", \"list\", input_size, output_size, num_units, \"ListToListModule\", device=device)\n","\n","    def init_model(self):\n","        self.model = RNNModule(self.input_size, self.output_size, self.num_units).to(self.device)\n","\n","    def execute_on_batch(self, batch, batch_lens):\n","        assert len(batch.size()) == 3\n","        model_out = self.model(batch, batch_lens)\n","        assert len(model_out.size()) == 3\n","        return model_out\n","\n","class ListToAtomModule(HeuristicNeuralFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        super().__init__(\"list\", \"atom\", input_size, output_size, num_units, \"ListToAtomModule\", device=device)\n","\n","    def init_model(self):\n","        self.model = RNNModule(self.input_size, self.output_size, self.num_units, device=self.device).to(device=self.device)\n","\n","    def execute_on_batch(self, batch, batch_lens, is_sequential=False):\n","        # print(batch.size())\n","        assert len(batch.size()) == 3\n","        model_out = self.model(batch, batch_lens)\n","        assert len(model_out.size()) == 3\n","\n","        if not is_sequential:\n","            idx = torch.tensor(batch_lens).to(self.device) - 1\n","            idx = idx.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, model_out.size(-1))\n","            model_out = model_out.gather(1, idx).squeeze(1)\n","\n","        return model_out\n","\n","class ListToAtomModuleConv1D(HeuristicNeuralFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, seq_len = 13, device=\"cpu\"):\n","        self.seq_len = seq_len\n","        super().__init__(\"list\", \"atom\", input_size, output_size, num_units, \"ListToAtomModuleConv1D\", device=device)\n","\n","    def __call__(self, batch):\n","        return self.execute_on_batch(batch, None)\n","\n","    def init_model(self):\n","        # in_channels and out_channels are the same size. This ensures that\n","        # a separate set of weights is learned per feature\n","        self.model = nn.Conv1d(self.input_size, self.input_size * self.output_size, self.seq_len,\n","                               groups=self.input_size).to(self.device)\n","\n","    def execute_on_batch(self, batch, batch_lens, is_sequential=False):\n","        assert len(batch.size()) == 3\n","        # batch is batch_size, seq_len, input_size, but need to pass in\n","        # batch_size, input_size, seq_len\n","        # print(batch.size())\n","        batch = batch.transpose(1, 2)\n","        # output is batch_size, input_size * output_size, 1\n","        model_out = self.model(batch).squeeze().view(-1, self.input_size, self.output_size)\n","        model_out = torch.sum(model_out, dim=1)\n","        return model_out.view(-1, self.output_size)\n","\n","class AtomToAtomModule(HeuristicNeuralFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        super().__init__(\"atom\", \"atom\", input_size, output_size, num_units, \"AtomToAtomModule\", device=device)\n","\n","    def init_model(self):\n","        self.model = FeedForwardModule(self.input_size, self.output_size, self.num_units, device=self.device)\n","\n","    def execute_on_batch(self, batch, batch_lens=None):\n","        assert len(batch.size()) == 2\n","        model_out = self.model(batch)\n","        assert len(model_out.size()) == 2\n","        return model_out\n","\n","\n","##############################\n","####### NEURAL MODULES #######\n","##############################\n","\n","\n","class RNNModule(nn.Module):\n","\n","    def __init__(self, input_size, output_size, num_units, num_layers=1, device=\"cpu\"):\n","        super(RNNModule, self).__init__()\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.rnn_size = num_units\n","        self.num_layers = num_layers\n","        self.rnn = nn.LSTM(self.input_size, self.rnn_size, num_layers=self.num_layers).to(device)\n","        self.out_layer = nn.Linear(self.rnn_size, self.output_size)\n","        self.device = device\n","\n","    def init_hidden(self, batch_size):\n","        ahid = torch.zeros(self.num_layers, batch_size, self.rnn_size)\n","        bhid = torch.zeros(self.num_layers, batch_size, self.rnn_size)\n","        ahid = ahid.requires_grad_(True)\n","        bhid = bhid.requires_grad_(True)\n","        hid = (ahid.to(self.device), bhid.to(self.device))\n","        return hid\n","\n","    def forward(self, batch, batch_lens):\n","        assert isinstance(batch, torch.Tensor)\n","        batch_size, seq_len, feature_dim = batch.size()\n","\n","        # pass through rnn\n","        hidden = self.init_hidden(batch_size)\n","        batch_packed = torch.nn.utils.rnn.pack_padded_sequence(batch, batch_lens, batch_first=True, enforce_sorted=False)\n","        self.rnn.flatten_parameters()\n","        out, hidden = self.rnn(batch_packed, hidden)\n","        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n","\n","        # pass through linear layer\n","        out = out.contiguous()\n","        out = out.view(-1, out.shape[2])\n","        out = self.out_layer(out)\n","        out = out.view(batch_size, seq_len, -1)\n","\n","        return out\n","\n","class FeedForwardModule(nn.Module):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        super(FeedForwardModule, self).__init__()\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.hidden_size = num_units\n","        self.first_layer = nn.Linear(self.input_size, self.hidden_size).to(device)\n","        self.out_layer = nn.Linear(self.hidden_size, self.output_size).to(device)\n","        self.device = device\n","\n","    def forward(self, current_input):\n","        assert isinstance(current_input, torch.Tensor)\n","        current_input = current_input.to(self.device)\n","        current = F.relu(self.first_layer(current_input))\n","        current = self.out_layer(current)\n","        return current"],"metadata":{"id":"6dLEFuTABnwd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generic Library Functions"],"metadata":{"id":"rf4u_1SrCYDM"}},{"cell_type":"code","source":["class LibraryFunction:\n","\n","    def __init__(self, submodules, input_type, output_type, input_size, \n","                 output_size, num_units, name=\"\", has_params=False, device=\"cpu\"):\n","        self.submodules = submodules\n","        self.input_type = input_type\n","        self.output_type = output_type\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.num_units = num_units\n","        self.name = name\n","        self.has_params = has_params\n","        self.device = device\n","\n","        if self.has_params:\n","            assert \"init_params\" in dir(self)\n","            self.init_params()\n","\n","    def get_submodules(self):\n","        return self.submodules\n","\n","    def set_submodules(self, new_submodules):\n","        self.submodules = new_submodules\n","\n","    def get_typesignature(self):\n","        return self.input_type, self.output_type\n","\n","class StartFunction(LibraryFunction):\n","\n","    def __init__(self, input_type, output_type, input_size, output_size, num_units, device=\"cpu\"):\n","        self.program = init_neural_function(input_type, output_type, input_size, output_size, num_units, device=device)\n","        submodules = { 'program' : self.program }\n","        super().__init__(submodules, input_type, output_type, input_size, \n","                         output_size, num_units, name=\"Start\", device=device)\n","\n","    def execute_on_batch(self, batch, batch_lens=None, batch_output=None, is_sequential=False):\n","        return self.submodules[\"program\"].execute_on_batch(batch, batch_lens)\n","            \n","class FoldFunction(LibraryFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, fold_function=None, device=\"cpu\"):\n","        #will this accumulator require a grad?\n","        self.accumulator = torch.zeros(output_size)\n","        if fold_function is None:\n","            fold_function = init_neural_function(\"atom\", \"atom\", input_size+output_size, output_size, num_units, device=device)\n","        submodules = { \"foldfunction\" : fold_function }\n","        super().__init__(submodules, \"list\", \"atom\", input_size, output_size, num_units, name=\"Fold\", device=device)\n","\n","    def execute_on_batch(self, batch, batch_lens=None, is_sequential=False):\n","        assert len(batch.size()) == 3\n","        batch_size, seq_len, feature_dim = batch.size()\n","        batch = batch.transpose(0,1) # (seq_len, batch_size, feature_dim)\n","\n","        fold_out = []\n","        folded_val = self.accumulator.clone().detach().requires_grad_(True)\n","        folded_val = folded_val.unsqueeze(0).repeat(batch_size,1).to(self.device)\n","        for t in range(seq_len):\n","            features = batch[t]\n","            out_val = self.submodules[\"foldfunction\"].execute_on_batch(torch.cat([features, folded_val], dim=1))\n","            fold_out.append(out_val.unsqueeze(1))\n","            folded_val = out_val\n","        fold_out = torch.cat(fold_out, dim=1)\n","        \n","        if not is_sequential:\n","            idx = torch.tensor(batch_lens).to(self.device) - 1\n","            idx = idx.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, fold_out.size(-1))\n","            fold_out = fold_out.gather(1, idx).squeeze(1)\n","\n","        return fold_out\n","\n","class MapFunction(LibraryFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, map_function=None, device=\"cpu\"):\n","        if map_function is None:\n","            map_function = init_neural_function(\"atom\", \"atom\", input_size, output_size, num_units, device=device)\n","        submodules = { \"mapfunction\" : map_function }\n","        super().__init__(submodules, \"list\", \"list\", input_size, output_size, num_units, name=\"Map\", device=device)\n","\n","    def execute_on_batch(self, batch, batch_lens=None):\n","        assert len(batch.size()) == 3\n","        batch_size, seq_len, feature_dim = batch.size()\n","        map_input = batch.view(-1, feature_dim)\n","        map_output = self.submodules[\"mapfunction\"].execute_on_batch(map_input)\n","        return map_output.view(batch_size, seq_len, -1)\n","\n","class MapPrefixesFunction(LibraryFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, map_function=None, device=\"cpu\"):\n","        if map_function is None:\n","            map_function = init_neural_function(\"list\", \"atom\", input_size, output_size, num_units, device=device)\n","        submodules = { \"mapfunction\" : map_function }\n","        super().__init__(submodules, \"list\", \"list\", input_size, output_size, num_units, name=\"MapPrefixes\", device=device)\n","\n","    def execute_on_batch(self, batch, batch_lens):\n","        assert len(batch.size()) == 3\n","        map_output = self.submodules[\"mapfunction\"].execute_on_batch(batch, batch_lens, is_sequential=True)\n","        assert len(map_output.size()) == 3\n","        return map_output\n","\n","class ITE(LibraryFunction):\n","    \"\"\"(Smoothed) If-The-Else.\"\"\"\n","\n","    def __init__(self, input_type, output_type, input_size, output_size, num_units, \n","                 eval_function=None, function1=None, function2=None, beta=1.0, name=\"ITE\", simple=False, device=\"cpu\"):\n","        if eval_function is None:\n","            if simple:\n","                eval_function = init_neural_function(input_type, \"atom\", input_size, 1, num_units, device=device)\n","            else:\n","                eval_function = init_neural_function(input_type, \"atom\", input_size, output_size, num_units, device=device)\n","        if function1 is None:\n","            function1 = init_neural_function(input_type, output_type, input_size, output_size, num_units, device=device)\n","        if function2 is None:\n","            function2 = init_neural_function(input_type, output_type, input_size, output_size, num_units, device=device)\n","        submodules = { \"evalfunction\" : eval_function, \"function1\" : function1, \"function2\" : function2 }\n","        self.bsmooth = nn.Sigmoid()\n","        self.beta = beta\n","        self.simple = simple # the simple version of ITE evaluates the same function for all dimensions of the output\n","        super().__init__(submodules, input_type, output_type, input_size, output_size, num_units, name=name, device=device)\n","\n","    def execute_on_batch(self, batch, batch_lens=None, is_sequential=False):\n","        if self.input_type == 'list':\n","            assert len(batch.size()) == 3\n","            assert batch_lens is not None\n","        else:\n","            assert len(batch.size()) == 2\n","        if is_sequential:\n","            predicted_eval = self.submodules[\"evalfunction\"].execute_on_batch(batch, batch_lens, is_sequential=False)\n","            predicted_function1 = self.submodules[\"function1\"].execute_on_batch(batch, batch_lens, is_sequential=is_sequential)\n","            predicted_function2 = self.submodules[\"function2\"].execute_on_batch(batch, batch_lens, is_sequential=is_sequential)\n","        else:\n","            predicted_eval = self.submodules[\"evalfunction\"].execute_on_batch(batch, batch_lens)\n","            predicted_function1 = self.submodules[\"function1\"].execute_on_batch(batch, batch_lens)\n","            predicted_function2 = self.submodules[\"function2\"].execute_on_batch(batch, batch_lens)\n","\n","        gate = self.bsmooth(predicted_eval*self.beta)\n","        if self.simple:\n","            gate = gate.repeat(1, self.output_size)\n","        \n","        if self.get_typesignature() == ('list', 'list'):\n","            gate = gate.unsqueeze(1).repeat(1, batch.size(1), 1)\n","        elif self.get_typesignature() == ('list', 'atom') and is_sequential:\n","            gate = gate.unsqueeze(1).repeat(1, batch.size(1), 1)\n","\n","        assert gate.size() == predicted_function2.size() == predicted_function1.size()\n","        ite_result = gate*predicted_function1 + (1.0 - gate)*predicted_function2\n","\n","        return ite_result\n","\n","class SimpleITE(ITE):\n","    \"\"\"The simple version of ITE evaluates one function for all dimensions of the output.\"\"\"\n","\n","    def __init__(self, input_type, output_type, input_size, output_size, num_units, eval_function=None, function1=None, function2=None, beta=1.0, device=\"cpu\"):\n","        super().__init__(input_type, output_type, input_size, output_size, num_units, \n","            eval_function=eval_function, function1=function1, function2=function2, beta=beta, name=\"SimpleITE\", simple=True, device=device)\n","        \n","class AndFunction(LibraryFunction):\n","\n","    def __init__(self, input_type, output_type, input_size, output_size, num_units, function1=None, function2=None, device=\"cpu\"):\n","        if function1 is None:\n","            function1 = init_neural_function(input_type, output_type, input_size, output_size, num_units, device=device)\n","        if function2 is None:\n","            function2 = init_neural_function(input_type, output_type, input_size, output_size, num_units, device=device)\n","        submodules = { \"function1\" : function1, \"function2\" : function2 }\n","        super().__init__(submodules, input_type, output_type, input_size, output_size, num_units, name=\"And\", device=device)\n","\n","    def execute_on_batch(self, batch, batch_lens=None, is_sequential=False):\n","        if self.input_type == 'list':\n","            assert len(batch.size()) == 3\n","            predicted_function1 = self.submodules[\"function1\"].execute_on_batch(batch, batch_lens=batch_lens,\n","                                                                                is_sequential=is_sequential)\n","            predicted_function2 = self.submodules[\"function2\"].execute_on_batch(batch, batch_lens=batch_lens,\n","                                                                                is_sequential=is_sequential)\n","        else:\n","            assert len(batch.size()) == 2\n","            predicted_function1 = self.submodules[\"function1\"].execute_on_batch(batch, batch_lens=batch_lens)\n","            predicted_function2 = self.submodules[\"function2\"].execute_on_batch(batch, batch_lens=batch_lens)\n","        return predicted_function1 * predicted_function2\n","\n","class OrFunction(LibraryFunction):\n","\n","    def __init__(self, input_type, output_type, input_size, output_size, num_units, function1=None, function2=None, device=\"cpu\"):\n","        if function1 is None:\n","            function1 = init_neural_function(input_type, output_type, input_size, output_size, num_units, device=device)\n","        if function2 is None:\n","            function2 = init_neural_function(input_type, output_type, input_size, output_size, num_units, device=device)\n","        submodules = { \"function1\": function1, \"function2\": function2 }\n","        super().__init__(submodules, input_type, output_type, input_size, output_size, num_units, name=\"Or\", device=device)\n","\n","    def execute_on_batch(self, batch, batch_lens=None, is_sequential=False):\n","        if self.input_type == 'list':\n","            assert len(batch.size()) == 3\n","            predicted_function1 = self.submodules[\"function1\"].execute_on_batch(batch, batch_lens=batch_lens,\n","                                                                                is_sequential=is_sequential)\n","            predicted_function2 = self.submodules[\"function2\"].execute_on_batch(batch, batch_lens=batch_lens,\n","                                                                                is_sequential=is_sequential)\n","        else:\n","            assert len(batch.size()) == 2\n","            predicted_function1 = self.submodules[\"function1\"].execute_on_batch(batch, batch_lens=batch_lens)\n","            predicted_function2 = self.submodules[\"function2\"].execute_on_batch(batch, batch_lens=batch_lens)\n","        return predicted_function1 + predicted_function2\n","\n","class ContinueFunction(LibraryFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, fxn=None, device=\"cpu\"):\n","        if fxn is None:\n","            fxn = init_neural_function(\"atom\", \"atom\", input_size, output_size, num_units, device=device)\n","        submodules = { \"fxn\" : fxn }\n","        super().__init__(submodules, \"atom\", \"atom\", input_size, output_size, num_units, name=\"\", device=device)\n","\n","    def execute_on_batch(self, batch, batch_lens=None):\n","        assert len(batch.size()) == 2\n","        fxn_out = self.submodules[\"fxn\"].execute_on_batch(batch)\n","        return fxn_out\n","\n","class LearnedConstantFunction(LibraryFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        super().__init__({}, \"atom\", \"atom\", input_size, output_size, num_units, name=\"LearnedConstant\", has_params=True, device=device)\n","\n","    def init_params(self):\n","        self.parameters = { \"constant\" : torch.rand(self.output_size, requires_grad=True, device=self.device) }\n","\n","    def execute_on_batch(self, batch, batch_lens=None):\n","        return self.parameters[\"constant\"].unsqueeze(0).repeat(batch.size(0), 1)\n","        \n","class AffineFunction(LibraryFunction):\n","\n","    def __init__(self, raw_input_size, selected_input_size, output_size, num_units, name=\"Affine\", device=\"cpu\"):\n","        self.selected_input_size = selected_input_size\n","        super().__init__({}, \"atom\", \"atom\", raw_input_size, output_size, \n","                         num_units, name=name, has_params=True, device=device)\n","\n","    def init_params(self):\n","        self.linear_layer = nn.Linear(self.selected_input_size, self.output_size, bias=True).to(self.device)\n","        self.parameters = {\n","            \"weights\" : self.linear_layer.weight,\n","            \"bias\" : self.linear_layer.bias\n","        }\n","\n","    def execute_on_batch(self, batch, batch_lens=None):\n","        assert len(batch.size()) == 2\n","        return self.linear_layer(batch)\n","\n","class AffineFeatureSelectionFunction(AffineFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, name=\"AffineFeatureSelection\", device=\"cpu\"):\n","        assert hasattr(self, \"full_feature_dim\")\n","        assert input_size >= self.full_feature_dim\n","        if self.full_feature_dim == 0:\n","            self.is_full = True\n","            self.full_feature_dim = input_size\n","        else:\n","            self.is_full = False\n","        additional_inputs = input_size - self.full_feature_dim\n","\n","        assert hasattr(self, \"feature_tensor\")\n","        assert len(self.feature_tensor) <= input_size\n","        self.feature_tensor = self.feature_tensor.to(device)\n","        super().__init__(raw_input_size=input_size, selected_input_size=self.feature_tensor.size()[-1]+additional_inputs, \n","            output_size=output_size, num_units=num_units, name=name, device=device)\n","\n","    def init_params(self):\n","        self.raw_input_size = self.input_size\n","        if self.is_full:\n","            self.full_feature_dim = self.input_size\n","            self.feature_tensor = torch.arange(self.input_size).to(self.device)\n","\n","        additional_inputs = self.raw_input_size - self.full_feature_dim\n","        self.selected_input_size = self.feature_tensor.size()[-1] + additional_inputs\n","        self.linear_layer = nn.Linear(self.selected_input_size, self.output_size, bias=True).to(self.device)\n","        self.parameters = {\n","            \"weights\" : self.linear_layer.weight,\n","            \"bias\" : self.linear_layer.bias\n","        }\n","\n","    def execute_on_batch(self, batch, batch_lens=None):\n","        assert len(batch.size()) == 2\n","        features = torch.index_select(batch, 1, self.feature_tensor)\n","        remaining_features = batch[:,self.full_feature_dim:]\n","        return self.linear_layer(torch.cat([features, remaining_features], dim=-1))\n","\n","class NeuralFeatureSelectionFunction(LibraryFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, name=\"NeuralFeatureSelection\", device=\"cpu\"):\n","\n","        self.feature_tensor = torch.arange(input_size)\n","        self.feature_tensor = self.feature_tensor.to(device)\n","        self.custom_nn = True\n","        hidden_dim = 32\n","\n","        self.model = nn.Sequential(\n","               nn.Linear(input_size,hidden_dim),\n","               nn.ReLU(),\n","               nn.Linear(hidden_dim,output_size)).to(device)\n","\n","        # def __init__(self, submodules, input_type, output_type, input_size, \n","        #              output_size, num_units, name=\"\", has_params=False, device=\"cpu\"):\n","\n","        super().__init__({}, 'atom', 'atom', input_size, output_size, \n","                         num_units=num_units, name=name, device=device)\n","\n","    def execute_on_batch(self, batch, batch_lens=None):\n","        assert len(batch.size()) == 2\n","        features = torch.index_select(batch, 1, self.feature_tensor)\n","\n","        return self.model(features)\n","\n","class FullInputAffineFunction(AffineFeatureSelectionFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        self.full_feature_dim = 0 # this will indicate additional_inputs = 0 in FeatureSelectionFunction\n","        self.feature_tensor = torch.arange(input_size) # selects all features by default\n","        super().__init__(input_size, output_size, num_units, name=\"FullFeatureSelect\", device=device)\n","\n","\n","class RunningAverageFunction(LibraryFunction):\n","    \"\"\"Computes running average over a window, then applies an Atom2AtomModule on the average.\"\"\"\n","\n","    def __init__(self, input_size, output_size, num_units, a2a_function=None, name=\"RunningAvg\", device=\"cpu\"):\n","        self.device = device\n","\n","        if a2a_function is None:\n","            a2a_function = init_neural_function(\"atom\", \"atom\", input_size, output_size, num_units, device = device)\n","        submodules = { \"subfunction\" : a2a_function }\n","        super().__init__(submodules, \"list\", \"atom\", input_size, output_size, num_units, name = name, device = device)\n","\n","    def window_start(self, t):\n","        return 0\n","\n","    def window_end(self, t):\n","        return t\n","\n","    def execute_on_batch(self, batch, batch_lens, is_sequential=False):\n","        assert len(batch.size()) == 3\n","        batch_size, seq_len, feature_dim = batch.size()\n","        batch = batch.transpose(0,1) # (seq_len, batch_size, feature_dim)\n","\n","        out = []\n","        for t in range(seq_len):\n","            window_start = max(0, self.window_start(t))\n","            window_end = min(seq_len, self.window_end(t))\n","            window = batch[window_start:window_end+1]\n","            running_average = torch.mean(window, dim=0)\n","            out_val = self.submodules[\"subfunction\"].execute_on_batch(running_average, \n","                                                                      batch_lens = batch_lens)\n","            out.append(out_val.unsqueeze(1))\n","        out = torch.cat(out, dim=1)        \n","        \n","        if not is_sequential:\n","            idx = torch.tensor(batch_lens).to(self.device) - 1\n","            idx = idx.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, out.size(-1))\n","            out = out.gather(1, idx).squeeze(1)\n","\n","        return out\n","\n","\n","class RunningAverageWindow5Function(RunningAverageFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, a2a_function=None, device=\"cpu\"):\n","        super().__init__(input_size, output_size, num_units, a2a_function, \n","                         name=\"Window5Avg\", device=device)\n","\n","    def window_start(self, t):\n","        return t-2\n","\n","    def window_end(self, t):\n","        return t+2\n","\n","class RunningAverageWindow7Function(RunningAverageFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, a2a_function=None, device=\"cpu\"):\n","        super().__init__(input_size, output_size, num_units, a2a_function, \n","                         name=\"Window7Avg\", device=device)\n","\n","    def window_start(self, t):\n","        return t-3\n","\n","    def window_end(self, t):\n","        return t+3\n","\n","\n","class RunningAverageWindow13Function(RunningAverageFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, a2a_function=None, device=\"cpu\"):\n","        super().__init__(input_size, output_size, num_units, a2a_function, \n","                         name=\"Window13Avg\", device=device)\n","\n","    def window_start(self, t):\n","        return t-6\n","\n","    def window_end(self, t):\n","        return t+6"],"metadata":{"id":"BVPTRGTUCXb5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Behavior Library Functions"],"metadata":{"id":"bWjBX6GvChkX"}},{"cell_type":"markdown","source":["### Mice Feature Subsets"],"metadata":{"id":"x9zsMi9FRcWJ"}},{"cell_type":"code","source":["CALMS21_FEATURE_SUBSETS = {\n","    \"res_angle_head_body\": torch.arange(0, 2, dtype=torch.long),\n","    \"axis_ratio\": torch.arange(2, 4, dtype=torch.long),\n","    \"speed\": torch.arange(4, 6, dtype=torch.long),\n","    \"acceleration\": torch.arange(6, 8, dtype=torch.long),\n","    \"tangential_velocity\": torch.arange(8, 10, dtype=torch.long),\n","    \"rel_angle_social\": torch.arange(10, 12, dtype=torch.long),\n","    \"angle_between\": torch.arange(12, 13, dtype=torch.long),\n","    \"facing_angle\": torch.arange(13, 15, dtype=torch.long),\n","    \"overlap_bboxes\": torch.arange(15, 16, dtype=torch.long),\n","    \"area_ellipse_ratio\": torch.arange(16, 17, dtype=torch.long),\n","    \"min_res_nose_dist\": torch.arange(17, 18, dtype=torch.long)\n","}\n","\n","# Indices used in a feature subset\n","CALMS21_FULL_FEATURE_DIM = 18\n","\n","class CALMS21ResAngleHeadBodySelection(AffineFeatureSelectionFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        self.full_feature_dim = CALMS21_FULL_FEATURE_DIM\n","        self.feature_tensor = CALMS21_FEATURE_SUBSETS[\"res_angle_head_body\"]\n","        super().__init__(input_size, output_size, num_units, name=\"ResAngleHeadBodySelect\", device=device)\n","\n","\n","class CALMS21AxisRatioSelection(AffineFeatureSelectionFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        self.full_feature_dim = CALMS21_FULL_FEATURE_DIM\n","        self.feature_tensor = CALMS21_FEATURE_SUBSETS[\"axis_ratio\"]\n","        super().__init__(input_size, output_size, num_units, name=\"AxisRatioSelect\", device=device)\n","\n","\n","class CALMS21SpeedSelection(AffineFeatureSelectionFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        self.full_feature_dim = CALMS21_FULL_FEATURE_DIM\n","        self.feature_tensor = CALMS21_FEATURE_SUBSETS[\"speed\"]\n","        super().__init__(input_size, output_size, num_units, name=\"SpeedSelect\", device=device)\n","\n","\n","class CALMS21TangentialVelocitySelection(AffineFeatureSelectionFunction):\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        self.full_feature_dim = CALMS21_FULL_FEATURE_DIM\n","        self.feature_tensor = CALMS21_FEATURE_SUBSETS[\"tangential_velocity\"]\n","        super().__init__(input_size, output_size, num_units, name=\"TangentialVelocitySelect\", device=device)\n","\n","\n","class CALMS21AccelerationSelection(AffineFeatureSelectionFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        self.full_feature_dim = CALMS21_FULL_FEATURE_DIM\n","        self.feature_tensor = CALMS21_FEATURE_SUBSETS[\"acceleration\"]\n","        super().__init__(input_size, output_size, num_units, name=\"AccelerationSelect\", device=device)\n","\n","\n","class CALMS21RelAngleSocialSelection(AffineFeatureSelectionFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        self.full_feature_dim = CALMS21_FULL_FEATURE_DIM\n","        self.feature_tensor = CALMS21_FEATURE_SUBSETS[\"rel_angle_social\"]\n","        super().__init__(input_size, output_size, num_units, name=\"RelativeSocialAngleSelect\", device=device)\n","\n","\n","class CALMS21AngleBetweenSelection(AffineFeatureSelectionFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        self.full_feature_dim = CALMS21_FULL_FEATURE_DIM\n","        self.feature_tensor = CALMS21_FEATURE_SUBSETS[\"angle_between\"]\n","        super().__init__(input_size, output_size, num_units, name=\"AngleBetweenSelect\", device=device)\n","\n","\n","class CALMS21FacingAngleSelection(AffineFeatureSelectionFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        self.full_feature_dim = CALMS21_FULL_FEATURE_DIM\n","        self.feature_tensor = CALMS21_FEATURE_SUBSETS[\"facing_angle\"]\n","        super().__init__(input_size, output_size, num_units, name=\"FacingAngleSelect\", device=device)\n","\n","\n","class CALMS21OverlapBboxesSelection(AffineFeatureSelectionFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        self.full_feature_dim = CALMS21_FULL_FEATURE_DIM\n","        self.feature_tensor = CALMS21_FEATURE_SUBSETS[\"overlap_bboxes\"]\n","        super().__init__(input_size, output_size, num_units, name=\"OverlapBboxesSelect\", device=device)\n","\n","\n","class CALMS21AreaEllipseRatioSelection(AffineFeatureSelectionFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        self.full_feature_dim = CALMS21_FULL_FEATURE_DIM\n","        self.feature_tensor = CALMS21_FEATURE_SUBSETS[\"area_ellipse_ratio\"]\n","        super().__init__(input_size, output_size, num_units, name=\"AreaEllipseRatioSelect\", device=device)\n","\n","\n","class CALMS21MinResNoseKeypointDistSelection(AffineFeatureSelectionFunction):\n","\n","    def __init__(self, input_size, output_size, num_units, device=\"cpu\"):\n","        self.full_feature_dim = CALMS21_FULL_FEATURE_DIM\n","        self.feature_tensor = CALMS21_FEATURE_SUBSETS[\"min_res_nose_dist\"]\n","        super().__init__(input_size, output_size, num_units, name=\"MinResNoseKeypointDistanceSelect\", device=device)"],"metadata":{"id":"MyAP0FTIChKV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Morlet Filter"],"metadata":{"id":"q2VtD9bBRjre"}},{"cell_type":"code","source":["import numpy as np\n","from abc import abstractmethod\n","\n","DEFAULT_RANGE = np.pi\n","\n","class SymmetricFilterOp(LibraryFunction):\n","    '''\n","    Template for an operation that first weighs a sequence of inputs\n","    by a symmetric filter and passes in the weighted average of the inputs\n","    to a submodule\n","    '''\n","    def __init__(self, input_size, output_size, num_units, function=None, name=\"SymFilterOp\", device=\"cpu\"):\n","        self.input_size = input_size\n","        # Default values for the parameters\n","        if function is None:\n","            function = init_neural_function(\"atom\", \"atom\", input_size, output_size, num_units, device=device)\n","        submodules = {\"function\": function}\n","        super().__init__(submodules, \"list\", \"atom\", input_size, output_size, num_units,\n","                         name=name, has_params=True, device=device)\n","\n","    @abstractmethod\n","    def init_params(self):\n","        \"\"\"Initializes filter parameters\"\"\"\n","        pass\n","    \n","    @abstractmethod\n","    def get_filter(self, xvals):\n","        \"\"\"Get filter weights on a sequence of input values\"\"\"\n","        pass\n","\n","    def get_filter_default_xvals(self, seq_len):\n","        \"\"\"Takes in seq_len, the number of linearly-spaced inputs. \n","        Get filter weights on xvals linearly spaced in [-DEFAULT_RANGE,\n","        DEFAULT_RANGE] \"\"\"\n","        xvals = torch.linspace(-DEFAULT_RANGE, DEFAULT_RANGE, seq_len).to(self.device)\n","        filter = self.get_filter(xvals)\n","        return filter\n","\n","    def execute_on_batch(self, batch, batch_lens=None, is_sequential=False):\n","        \"\"\"Apply the temporal filter to a batch of inputs\"\"\"\n","        assert len(batch.size()) == 3\n","        batch_size, seq_len, feature_dim = batch.size()\n","        filter = self.get_filter_default_xvals(seq_len)\n","        # Repeat a (seq_len, 1) vector to get (seq_len, feature_dim)\n","        filter = filter.repeat(1, feature_dim)\n","        # Apply the filter, sum to get (batch_size, feature_dim) and execute\n","        if not is_sequential:\n","            input = torch.sum(torch.mul(batch, filter), 1)\n","        else:\n","            input = torch.sum(torch.mul(batch, filter), 1).repeat(1, seq_len, 1).view(-1, feature_dim)\n","        output = self.submodules[\"function\"].execute_on_batch(input)\n","        return output\n","\n","\n","class MorletFilterOp(SymmetricFilterOp):\n","    '''\n","    The symmetric filter operation where the shape of the filter is defined\n","    by the morlet filter\n","    '''\n","\n","    def __init__(self, input_size, output_size, num_units, function=None, name=\"MorletFilterOp\", device=\"cpu\"):\n","        super().__init__(input_size, output_size, num_units, function=function,\n","                         name=name, device=device)\n","\n","    def init_params(self):\n","        \"\"\"Initialize Morlet Filter parameters\"\"\"\n","        self.parameters = {\n","            \"s\": torch.tensor(0.5, requires_grad=True, device=self.device),\n","            \"w\": torch.tensor(0.5, requires_grad=True, device=self.device)\n","        }\n","\n","    def get_mor_filter(self, xvals, s, w):\n","        \"\"\"Implements the Morlet Filter by applying the wavelet function \n","        psi to xvals \n","        \n","        Make sure to return the results with shape [seq_len, 1] instead \n","        of [seq_len]. view() or reshape(), as well as math functions in \n","        the torch library, e.g. torch.exp and torch.pow, should be helpful. \n","        \"\"\"\n","        return (torch.exp(-0.5 * torch.pow(w * xvals / s, 2)) * \\\n","                torch.cos(w * xvals)).view(-1, 1)\n","\n","    def get_filter(self, xvals):\n","        s = self.parameters[\"s\"]\n","        w = self.parameters[\"w\"]\n","        return self.get_mor_filter(xvals, s, w)\n","\n","\n","class AsymmetricFilterOp(SymmetricFilterOp):\n","    '''\n","    Template for an operation that first weighs a sequence of inputs\n","    by a filter that has an asymmetric shape and passes in the weighted \n","    average of the inputs to a submodule\n","    '''\n","\n","    def __init__(self, input_size, output_size, num_units, function=None, name=\"AsymFilterOp\", device=\"cpu\"):\n","        super().__init__(input_size, output_size, num_units, function=function,\n","                        name=name, device=device)\n","\n","    @abstractmethod\n","    def init_params(self):\n","        \"\"\"Initializes filter parameters\"\"\"\n","        pass\n","    \n","    @abstractmethod\n","    def get_filter(self, xvals, left=True):\n","        \"\"\"Get filter weights corresponding to a sequence of inputs, xvals. \n","        \n","        The shape of the filter depends on whether xvals are \n","        on the left side of the filter, i.e. left = True, or not\"\"\"\n","        pass\n","\n","    def get_filter_default_xvals(self, seq_len):\n","        \"\"\"\n","        Takes in seq_len, the total number of linearly-spaced inputs. \n","\n","        Get the left filter weights on (seq_len / 2) linearly-spaced xvals \n","        in [-pi, 0]. Get the right filter weights on the remaining \n","        linearly-spaced xvals in [0, pi].\n","        \n","        Return the left results concatenated with the right results.\n","\n","        \"\"\"\n","        # Implement for asymetric Morlet filter (note filter shape)\n","        # changes from left = True to left = False\n","        raise NotImplementedError\n","\n","\n","class AsymMorletFilterOp(AsymmetricFilterOp, MorletFilterOp):\n","    def __init__(self, input_size, output_size, num_units, function=None, name=\"AsymMorletFilterOp\", device=\"cpu\"):\n","        super().__init__(input_size, output_size, num_units, function=function, \n","                         name=name, device=device)\n","\n","    def init_params(self):\n","        \"\"\"s1 and w1 are parameters for the left side of the filter, s2\n","        and w2 are for the right side of the filter \"\"\"\n","        self.parameters = {\n","            \"s1\": torch.tensor(0.5, requires_grad=True, device=self.device),\n","            \"w1\": torch.tensor(0.5, requires_grad=True, device=self.device),\n","            \"s2\": torch.tensor(0.5, requires_grad=True, device=self.device),\n","            \"w2\": torch.tensor(0.5, requires_grad=True, device=self.device)\n","        }\n","\n","    def get_filter(self, xvals, left=True):\n","        s = self.parameters[\"s1\"]\n","        w = self.parameters[\"w1\"]\n","        if not left:\n","            s = self.parameters[\"s2\"]\n","            w = self.parameters[\"w2\"]\n","        return self.get_mor_filter(xvals, s, w)"],"metadata":{"id":"cDdVr72TRmxD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test Morlet Filter\n","\n","To check your implementation in the Morlet Filter section, run  the below tests, which will output 3 plots. Check that these plots match the MorletFilter.png file given in the near folder."],"metadata":{"id":"dcjXz865G3mo"}},{"cell_type":"code","source":["## Test the morlet filter\n","import matplotlib.pyplot as plt\n","\n","if __name__ == \"__main__\":\n","    mor_filter = MorletFilterOp(1, 1, 1)  # Just set input, output, num_units to any value\n","    mor_filter.parameters[\"s\"] = 0.60\n","    mor_filter.parameters[\"w\"] = 0.30\n","    weights = mor_filter.get_filter_default_xvals(13)\n","    weights = weights.detach().numpy()\n","    plt.figure()\n","    plt.plot(weights, color=\"orange\")\n","    plt.show()\n","\n","    asym_mor_filter = AsymMorletFilterOp(1, 1, 1)  # Just set input, output, num_units to any value\n","    asym_mor_filter.parameters[\"s2\"] = 0.75\n","    asym_mor_filter.parameters[\"w2\"] = 0.25\n","    weights = asym_mor_filter.get_filter_default_xvals(13)\n","    weights = weights.detach().numpy()\n","    plt.figure()\n","    plt.plot(weights, color=\"blue\")\n","    plt.show()\n","\n","    asym_mor_filter = AsymMorletFilterOp(1, 1, 1)  # Just set input, output, num_units to any value\n","    asym_mor_filter.parameters[\"s1\"] = 0.75\n","    asym_mor_filter.parameters[\"w1\"] = 0.25\n","    weights = asym_mor_filter.get_filter_default_xvals(13)\n","    weights = weights.detach().numpy()\n","    plt.figure()\n","    plt.plot(weights, color=\"red\")\n","    plt.show()"],"metadata":{"id":"acn4re5mzV79"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Starting DSL Dictionary"],"metadata":{"id":"E4kbZOsO6gX2"}},{"cell_type":"code","source":["DSL_DICT = {('list', 'list') : [MapFunction, SimpleITE],\n","                        ('list', 'atom') : [OrFunction, AndFunction, RunningAverageWindow5Function],\n","('atom', 'atom') : [SimpleITE, CALMS21ResAngleHeadBodySelection, \\\n","                    CALMS21SpeedSelection, CALMS21TangentialVelocitySelection, \\\n","                    CALMS21AccelerationSelection, CALMS21RelAngleSocialSelection, \\\n","                    CALMS21AxisRatioSelection, CALMS21OverlapBboxesSelection,\n","                    CALMS21MinResNoseKeypointDistSelection]}\n","\n","# If not updated, the default cost is based on the number of neural modules \n","# in the function\n","CUSTOM_EDGE_COSTS = {\n","    ('list', 'list') : {},\n","    ('list', 'atom') : {},\n","    ('atom', 'atom') : {}\n","}"],"metadata":{"id":"vgTZ45t66idh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DSL with Morlet Filter\n","\n","Create a new DSL called DSL_DICT_MOR which replaces the RunningAverage with MorletFilterOp in the original DSL_DICT."],"metadata":{"id":"pGKeHeS1Osph"}},{"cell_type":"code","source":["DSL_DICT_MOR = {('list', 'list') : [MapFunction, SimpleITE],\n","                        ('list', 'atom') : [OrFunction, AndFunction,\n","                                            MorletFilterOp],\n","('atom', 'atom') : [SimpleITE, CALMS21ResAngleHeadBodySelection, \\\n","                    CALMS21SpeedSelection, CALMS21TangentialVelocitySelection, \\\n","                    CALMS21AccelerationSelection, CALMS21RelAngleSocialSelection, \\\n","                    CALMS21AxisRatioSelection, CALMS21OverlapBboxesSelection,\n","                    CALMS21MinResNoseKeypointDistSelection]}\n"],"metadata":{"id":"KambDAIyNs_M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DSL with Neural Module\n","\n","Create a new DSL called DSL_DICT_NEUROSYM which replaces feature selects with learning a neural module over all features."],"metadata":{"id":"_BdzMbLQnxVJ"}},{"cell_type":"code","source":["DSL_DICT_NEUROSYM = {('list', 'list') : [MapFunction, SimpleITE],\n","                        ('list', 'atom') : [OrFunction, AndFunction, MorletFilterOp],\n","('atom', 'atom') : [SimpleITE, CALMS21ResAngleHeadBodySelection, \\\n","                    CALMS21SpeedSelection, CALMS21TangentialVelocitySelection, \\\n","                    CALMS21AccelerationSelection, CALMS21RelAngleSocialSelection, \\\n","                    CALMS21AxisRatioSelection, CALMS21OverlapBboxesSelection,\n","                    CALMS21MinResNoseKeypointDistSelection, NeuralFeatureSelectionFunction]}\n"],"metadata":{"id":"EEkalCjnnxfa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DSL with Asymmetric Morlet Filter\n","\n","Create a new DSL called DSL_DICT_ASYM_MOR which uses your new implementation of the asymmetric Morlet Filter."],"metadata":{"id":"gA8m7LKndCTj"}},{"cell_type":"code","source":["DSL_DICT_ASYM_MOR = {('list', 'list') : [MapFunction, SimpleITE],\n","                        ('list', 'atom') : [OrFunction, AndFunction,\n","                                            AsymMorletFilterOp],\n","('atom', 'atom') : [SimpleITE, CALMS21ResAngleHeadBodySelection, \\\n","                    CALMS21SpeedSelection, CALMS21TangentialVelocitySelection, \\\n","                    CALMS21AccelerationSelection, CALMS21RelAngleSocialSelection, \\\n","                    CALMS21AxisRatioSelection, CALMS21OverlapBboxesSelection,\n","                    CALMS21MinResNoseKeypointDistSelection]}\n"],"metadata":{"id":"c0miU9ASdBJ_"},"execution_count":null,"outputs":[]}]}