{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"near.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## NEAR Code"],"metadata":{"id":"YcApgK3fAt49"}},{"cell_type":"code","source":["import argparse\n","import os\n","import logging\n","import pickle\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import copy\n","import time\n","\n","from dsl import StartFunction, ITE, HeuristicNeuralFunction, LibraryFunction, \\\n","                ListToListModule, ListToAtomModule, OrFunction, AndFunction\n","from collections.abc import Iterable"],"metadata":{"id":"aSc8GTLM-gZ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Utilities"],"metadata":{"id":"D3rWQuTHKFrN"}},{"cell_type":"markdown","source":["### Data Processing"],"metadata":{"id":"SQJqtDE7L9G9"}},{"cell_type":"code","source":["def flatten_batch(batch):\n","    if not isinstance(batch[0], Iterable) or len(batch[0]) == 1:\n","        return batch\n","    new_batch = []\n","    for traj_list in batch:\n","        new_batch.extend(traj_list)\n","    return new_batch\n","\n","def flatten_tensor(batch_out):\n","    return torch.cat(batch_out)\n","\n","def pad_minibatch(minibatch, num_features=-1, pad_token=-1, return_max=False):\n","    #minibatch to be of dimension [num_sequences, num_features, len of each sequence (variable)]\n","    #adapted from Will Falcon's blog post\n","    batch_lengths = [len(sequence) for sequence in minibatch]\n","    batch_size = len(minibatch)\n","    longest_seq = max(batch_lengths)\n","    padded_minibatch = torch.ones((batch_size, longest_seq, num_features)) * pad_token\n","    for i, seq_len in enumerate(batch_lengths):\n","        seq = minibatch[i]\n","        if num_features == 1:\n","            seq = seq.unsqueeze(1)\n","        padded_minibatch[i, 0:seq_len] = seq[:seq_len]\n","    if return_max:\n","        return padded_minibatch, batch_lengths, longest_seq\n","    else:\n","        return padded_minibatch, batch_lengths\n","\n","def unpad_minibatch(minibatch, lengths, listtoatom=False):\n","    new_minibatch = []\n","    for idx, length in enumerate(lengths):\n","        if listtoatom:\n","            new_minibatch.append(minibatch[idx][length-1])\n","        else:\n","            new_minibatch.append(minibatch[idx][:length])\n","    return new_minibatch\n","\n","def dataset_tolists(trajs, labels):\n","    assert len(trajs) == len(labels)\n","\n","    dataset = []\n","    for k, traj in enumerate(trajs):\n","        traj_list = []\n","        for t in range(len(traj)):\n","            traj_list.append(traj[t])\n","\n","        label = torch.tensor(labels[k]).long()\n","        dataset.append([traj_list, label])\n","\n","    return dataset\n","\n","def normalize_data(train_data, valid_data, test_data):\n","    \"\"\"Normalize features wrt. mean and std of training data.\"\"\"\n","    _, seq_len, input_dim = train_data.shape\n","    train_data_reshape = np.reshape(train_data, (-1, input_dim))\n","    test_data_reshape = np.reshape(test_data, (-1, input_dim))\n","    features_mean = np.mean(train_data_reshape, axis=0)\n","    features_std = np.std(train_data_reshape, axis=0)\n","    train_data_reshape = (train_data_reshape - features_mean) / features_std\n","    test_data_reshape = (test_data_reshape - features_mean) / features_std\n","    train_data = np.reshape(train_data_reshape, (-1, seq_len, input_dim))\n","    test_data = np.reshape(test_data_reshape, (-1, seq_len, input_dim))\n","    if valid_data is not None:\n","        valid_data_reshape = np.reshape(valid_data, (-1, input_dim))\n","        valid_data_reshape = (valid_data_reshape - features_mean) / features_std\n","        valid_data = np.reshape(valid_data_reshape, (-1, seq_len, input_dim))\n","    return train_data, valid_data, test_data\n","\n","def create_minibatches(all_items, batch_size):\n","    num_items = len(all_items)\n","    batches = []\n","    def create_single_minibatch(idxseq):\n","        curr_batch = []\n","        for idx in idxseq:\n","            curr_batch.append((all_items[idx]))\n","        return curr_batch\n","    item_idxs = list(range(num_items))\n","    while len(item_idxs) > 0:\n","        if len(item_idxs) <= batch_size:\n","            batch = create_single_minibatch(item_idxs)\n","            batches.append(batch)\n","            item_idxs = []\n","        else:\n","            # get batch indices\n","            batchidxs = []\n","            while len(batchidxs) < batch_size:\n","                rando = random.randrange(len(item_idxs))\n","                index = item_idxs.pop(rando)\n","                batchidxs.append(index)\n","            batch = create_single_minibatch(batchidxs)\n","            batches.append(batch)\n","    return batches\n","\n","def prepare_datasets(train_data, valid_data, test_data, train_labels, valid_labels, test_labels, normalize=True, train_valid_split=0.7, batch_size=32):\n","    if normalize:\n","        train_data, valid_data, test_data = normalize_data(train_data, valid_data, test_data)\n","\n","    trainset = dataset_tolists(train_data, train_labels) \n","    testset = dataset_tolists(test_data, test_labels)\n","\n","    if valid_data is not None and valid_labels is not None:\n","        validset = dataset_tolists(valid_data, valid_labels)\n","    # Split training for validation set if validation set is not provided.\n","    elif train_valid_split < 1.0:\n","        split = int(train_valid_split*len(train_data))\n","        validset = trainset[split:]\n","        trainset = trainset[:split]\n","    else:\n","        split = int(train_valid_split)\n","        validset = trainset[split:]\n","        trainset = trainset[:split]\n","\n","    # Create minibatches for training\n","    batched_trainset = create_minibatches(trainset, batch_size)\n","\n","    return batched_trainset, validset, testset"],"metadata":{"id":"GAysKoJBLPaa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Logging"],"metadata":{"id":"FFFDns9j_qJa"}},{"cell_type":"code","source":["def init_logging(save_path):\n","    logfile = os.path.join(save_path, 'log.txt')\n","\n","    # clear log file\n","    with open(logfile, 'w'):\n","        pass\n","    # remove previous handlers\n","    for handler in logging.root.handlers[:]:\n","        logging.root.removeHandler(handler)\n","    logging.basicConfig(filename=logfile, level=logging.INFO)\n","\n","def log_and_print(line):\n","    print(line)\n","    logging.info(line)\n","\n","def log(line):\n","    logging.info(line)\n","\n","def print_program(program, ignore_constants=True):\n","    if not isinstance(program, LibraryFunction):\n","        return program.name\n","    else:\n","        collected_names = []\n","        for submodule, functionclass in program.submodules.items():\n","            collected_names.append(print_program(functionclass, ignore_constants=ignore_constants))\n","        # log_and_print(program.has_params)\n","        if program.has_params:\n","            parameters = \"params: {}\".format(program.parameters.values())\n","            if not ignore_constants:\n","                collected_names.append(parameters)\n","        joined_names = ', '.join(collected_names)\n","        return program.name + \"(\" + joined_names + \")\"\n","\n","def print_program_dict(prog_dict):\n","    log_and_print(print_program(prog_dict[\"program\"], ignore_constants=True))\n","    log_and_print(\"struct_cost {:.4f} | score {:.4f} | path_cost {:.4f} | time {:.4f}\".format(\n","        prog_dict[\"struct_cost\"], prog_dict[\"score\"], prog_dict[\"path_cost\"], prog_dict[\"time\"]))"],"metadata":{"id":"dNw9_uiZPpjS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"4LY7j-lRMB-y"}},{"cell_type":"code","source":["def init_optimizer(program, optimizer, lr):\n","    queue = [program]\n","    all_params = []\n","    while len(queue) != 0:\n","        current_function = queue.pop()\n","        if issubclass(type(current_function), HeuristicNeuralFunction):\n","            current_function.init_model()\n","            all_params.append({'params' : current_function.model.parameters(),'lr' : lr})\n","        elif current_function.has_params:\n","            if current_function.parameters is None:\n","                current_function.init_params()\n","            all_params.append({'params': list(current_function.parameters.values()), 'lr': lr})\n","            for submodule, functionclass in current_function.submodules.items():\n","                queue.append(functionclass)\n","        elif 'custom_nn' in current_function.__dict__ and current_function.custom_nn:\n","            all_params.append({'params' : current_function.model.parameters(),'lr' : lr})\n","        else:\n","            for submodule, functionclass in current_function.submodules.items():\n","                queue.append(functionclass)\n","    curr_optim = optimizer(all_params, lr)\n","    return curr_optim\n","\n","def process_batch(program, batch, output_type, output_size, device='cpu'):\n","    batch_input = [torch.tensor(traj) for traj in batch]\n","    batch_padded, batch_lens = pad_minibatch(batch_input, num_features=batch_input[0].size(1))\n","    batch_padded = batch_padded.to(device)\n","    out_padded = program.execute_on_batch(batch_padded, batch_lens)\n","    if output_type == \"list\":\n","        out_unpadded = unpad_minibatch(out_padded, batch_lens, listtoatom=(program.output_type=='atom'))\n","    else:\n","        out_unpadded = out_padded\n","    if output_size == 1 or output_type == \"list\":\n","        if not isinstance(out_unpadded, list):\n","            return out_unpadded.squeeze(dim=1)\n","        return flatten_tensor(out_unpadded).squeeze()\n","    else:\n","        if isinstance(out_unpadded, list):\n","            out_unpadded = torch.cat(out_unpadded, dim=0).to(device)          \n","        return out_unpadded\n","\n","def execute_and_train(program, validset, trainset, train_config, output_type, output_size, \n","    neural=False, device='cpu', use_valid_score=False, print_every=60):\n","\n","    lr = train_config['lr']\n","    neural_epochs = train_config['neural_epochs']\n","    symbolic_epochs = train_config['symbolic_epochs']\n","    optimizer = train_config['optimizer']\n","    lossfxn = train_config['lossfxn']\n","    evalfxn = train_config['evalfxn']\n","    num_labels = train_config['num_labels']\n","\n","    num_epochs = neural_epochs if neural else symbolic_epochs\n","\n","    # initialize optimizer\n","    curr_optim = init_optimizer(program, optimizer, lr)\n","\n","    # prepare validation set\n","    validation_input, validation_output = map(list, zip(*validset))\n","    validation_true_vals = torch.tensor(flatten_batch(validation_output)).float().to(device)\n","    # TODO a little hacky, but easiest solution for now\n","    if isinstance(lossfxn, nn.CrossEntropyLoss):\n","        validation_true_vals = validation_true_vals.long()\n","\n","    best_program = None\n","    best_metric = float('inf')\n","    best_additional_params = {}\n","\n","    for epoch in range(1, num_epochs+1):\n","        for batchidx in range(len(trainset)):\n","            batch_input, batch_output = map(list, zip(*trainset[batchidx]))\n","            true_vals = torch.tensor(flatten_batch(batch_output)).float().to(device)\n","            predicted_vals = process_batch(program, batch_input, output_type, output_size, device)\n","            # TODO a little hacky, but easiest solution for now\n","            if isinstance(lossfxn, nn.CrossEntropyLoss):\n","                true_vals = true_vals.long()\n","            #print(predicted_vals.shape, true_vals.shape)\n","            loss = lossfxn(predicted_vals, true_vals)\n","            curr_optim.zero_grad()\n","            loss.backward()\n","            curr_optim.step()\n","\n","            # if batchidx % print_every == 0 or batchidx == 0:\n","            #     log_and_print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch, num_epochs, loss.item()))\n","\n","        # check score on validation set\n","        with torch.no_grad():\n","            predicted_vals = process_batch(program, validation_input, output_type, output_size, device)\n","            metric, additional_params = evalfxn(predicted_vals, validation_true_vals, num_labels=num_labels)\n","\n","        if use_valid_score:\n","            if metric < best_metric:\n","                best_program = copy.deepcopy(program)\n","                best_metric = metric\n","                best_additional_params = additional_params\n","        else:\n","            best_program = copy.deepcopy(program)\n","            best_metric = metric\n","            best_additional_params = additional_params\n","\n","    # select model with best validation score\n","    program = copy.deepcopy(best_program)\n","    log(\"Validation score is: {:.4f}\".format(best_metric))\n","    log(\"Average f1-score is: {:.4f}\".format(1 - best_metric))\n","    log(\"Hamming accuracy is: {:.4f}\".format(best_additional_params['hamming_accuracy']))\n","    \n","    return best_metric"],"metadata":{"id":"qAf1xq1mLUc8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"id":"_K1dwSEWP_B2"}},{"cell_type":"code","source":["from sklearn.metrics import hamming_loss, f1_score\n","\n","def compute_average_f1_score(predicted, truth, num_labels):\n","    assert isinstance(predicted, torch.Tensor)\n","    assert isinstance(truth, torch.Tensor)\n","\n","    if num_labels > 1:\n","        weighted_avg_f1 = f1_score(truth, predicted, average='weighted')\n","        unweighted_avg_f1 = f1_score(truth, predicted, average='macro')\n","        all_f1 = f1_score(truth, predicted, average=None)\n","        return weighted_avg_f1, unweighted_avg_f1, all_f1\n","    else:\n","        avg_f1 = f1_score(truth, predicted, average='binary')\n","        all_f1 = f1_score(truth, predicted, average=None)\n","        return avg_f1, all_f1\n","\n","def label_correctness(predictions, truths, num_labels=1):\n","    #counts up hamming distance and true accuracy\n","    # assert predictions.size(-1) == num_labels\n","\n","    additional_scores = {}\n","    if len(predictions.size()) == 1:\n","        predictions = torch.sigmoid(predictions) > 0.5\n","    else:\n","        assert len(predictions.size()) == 2\n","        predictions = torch.max(predictions, dim=-1)[1]\n","\n","    additional_scores['hamming_accuracy'] = 1 - hamming_loss(truths.squeeze().cpu(), predictions.squeeze().cpu())\n","    if num_labels > 1:\n","        w_avg_f1, additional_scores['unweighted_f1'], additional_scores['all_f1s'] = compute_average_f1_score(truths.squeeze().cpu(), predictions.squeeze().cpu(), num_labels)\n","        return 1 - w_avg_f1, additional_scores\n","    else:\n","        w_avg_f1, additional_scores['all_f1s'] = compute_average_f1_score(truths.squeeze().cpu(), predictions.squeeze().cpu(), num_labels)\n","        return 1 - w_avg_f1, additional_scores\n","\n","def test_set_eval(program, testset, output_type, output_size, num_labels, device='cpu', verbose=False):\n","    log_and_print(\"\\n\")\n","    log_and_print(\"Evaluating program {} on TEST SET\".format(print_program(program, ignore_constants=(not verbose))))\n","    with torch.no_grad():\n","        test_input, test_output = map(list, zip(*testset))\n","        true_vals = torch.tensor(flatten_batch(test_output)).to(device)\n","        predicted_vals = process_batch(program, test_input, output_type, output_size, device)\n","        metric, additional_params = label_correctness(predicted_vals, true_vals, num_labels=num_labels)\n","    log_and_print(\"F1 score achieved is {:.4f}\".format(1 - metric))\n","    log_and_print(\"Additional performance parameters: {}\\n\".format(additional_params))"],"metadata":{"id":"zv3bvQ6fP-jZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Program Graph"],"metadata":{"id":"BkCVjX-C-vbx"}},{"cell_type":"code","source":["class ProgramNode(object):\n","\n","    def __init__(self, program, score, parent, depth, cost, order):\n","        self.score = score\n","        self.program = program\n","        self.children = []\n","        self.parent = parent\n","        self.depth = depth\n","        self.cost = cost\n","        self.order = order\n","\n","\n","class ProgramGraph(object):\n","\n","    def __init__(self, dsl_dict, edge_cost_dict, input_type, output_type, input_size, output_size,\n","        max_num_units, min_num_units, max_num_children, max_depth, penalty, ite_beta=1.0, device=\"cpu\"):\n","        self.dsl_dict = dsl_dict\n","        self.edge_cost_dict = edge_cost_dict\n","        self.input_type = input_type\n","        self.output_type = output_type\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.max_num_units = max_num_units\n","        self.min_num_units = min_num_units\n","        self.max_num_children = max_num_children\n","        self.max_depth = max_depth\n","        self.penalty = penalty\n","        self.ite_beta = ite_beta\n","        self.device = device\n","        start = StartFunction(input_type=input_type, output_type=output_type, \n","            input_size=input_size, output_size=output_size, num_units=max_num_units, device=self.device)\n","        self.root_node = ProgramNode(start, 0, None, 0, 0, 0)\n","\n","    def construct_candidates(self, input_type, output_type, input_size, output_size, num_units):\n","        candidates = []\n","        replacement_candidates = self.dsl_dict[(input_type, output_type)]\n","        for functionclass in replacement_candidates:\n","            if issubclass(functionclass, ITE):\n","                candidate = functionclass(input_type, output_type, input_size, output_size, num_units, beta=self.ite_beta, device=self.device)\n","            elif issubclass(functionclass, OrFunction) or issubclass(functionclass, AndFunction):\n","                candidate = functionclass(input_type, output_type, input_size, output_size, num_units, device=self.device)\n","            else:\n","                candidate = functionclass(input_size, output_size, num_units, device=self.device)\n","            candidates.append(candidate)\n","        return candidates\n","\n","    def is_fully_symbolic(self, candidate_program):\n","        queue = [(candidate_program.submodules['program'])]\n","        while len(queue) != 0:\n","            current_function = queue.pop()\n","            if issubclass(type(current_function), HeuristicNeuralFunction):\n","                return False\n","            else:\n","                for submodule in current_function.submodules:\n","                    queue.append(current_function.submodules[submodule])\n","        return True\n","\n","    def compute_edge_cost(self, expandion_candidate):\n","        edge_cost = 0\n","        functionclass = type(expandion_candidate)\n","        typesig = expandion_candidate.get_typesignature()\n","\n","        if functionclass in self.edge_cost_dict[typesig]:\n","            edge_cost = self.edge_cost_dict[typesig][functionclass]\n","        else:\n","            # Otherwise, the edge cost scales with the number of HeuristicNeuralFunction\n","            for submodule, fxnclass in expandion_candidate.submodules.items():\n","                if isinstance(fxnclass, HeuristicNeuralFunction):\n","                    edge_cost += 1\n","\n","        return edge_cost*self.penalty\n","\n","    def compute_program_cost(self, candidate_program):\n","        queue = [candidate_program.submodules['program']]\n","        total_cost = 0\n","        depth = 0\n","        edge_cost = 0\n","        while len(queue) != 0:\n","            depth += 1\n","            current_function = queue.pop()\n","            current_type = type(current_function)\n","            current_type_sig = current_function.get_typesignature()\n","            if current_type in self.edge_cost_dict[current_type_sig]:\n","                edge_cost = self.edge_cost_dict[current_type_sig][current_type]\n","            else:\n","                edge_cost = 0\n","                # Otherwise, the edge cost scales with the number of neural modules\n","                for submodule, fxnclass in current_function.submodules.items():\n","                    edge_cost += 1\n","            total_cost += edge_cost\n","            for submodule, functionclass in current_function.submodules.items():\n","                queue.append(functionclass)\n","        return total_cost * self.penalty, depth\n","\n","    def min_depth2go(self, candidate_program):\n","        depth2go = 0\n","        queue = [(candidate_program.submodules['program'])]\n","        while len(queue) != 0:\n","            current_function = queue.pop()\n","            if issubclass(type(current_function), HeuristicNeuralFunction):\n","                depth2go += 1\n","                # in current DSL, ListToList/ListToAtom both need at least 2 depth to be fully symbolic\n","                if issubclass(type(current_function), ListToListModule):\n","                    depth2go += 1\n","                elif issubclass(type(current_function), ListToAtomModule):\n","                    depth2go += 1\n","            else:\n","                for submodule in current_function.submodules:\n","                    queue.append(current_function.submodules[submodule])\n","        return depth2go\n","\n","    def num_units_at_depth(self, depth):\n","        num_units = max(int(self.max_num_units*(0.5**(depth-1))), self.min_num_units)\n","        return num_units\n","\n","    def get_all_children(self, current_node, in_enumeration=False):\n","        all_children = []\n","        child_depth = current_node.depth + 1\n","        child_num_units = self.num_units_at_depth(child_depth)\n","        queue = [current_node.program]\n","        while len(queue) != 0:\n","            current = queue.pop()\n","            for submod, functionclass in current.submodules.items():\n","                if issubclass(type(functionclass), HeuristicNeuralFunction):\n","                    replacement_candidates = self.construct_candidates(functionclass.input_type,\n","                                                                   functionclass.output_type,\n","                                                                   functionclass.input_size,\n","                                                                   functionclass.output_size,\n","                                                                   child_num_units)\n","                    orig_fclass = copy.deepcopy(current.submodules[submod])\n","                    for child_candidate in replacement_candidates:\n","                        # replace the neural function with a candidate\n","                        current.submodules[submod] = child_candidate\n","                        # create the correct child node\n","                        child_node = copy.deepcopy(current_node)\n","                        child_node.depth = child_depth\n","                        # check if child program can be completed within max_depth\n","                        if child_node.depth + self.min_depth2go(child_node.program) > self.max_depth:\n","                            continue\n","                        # if yes, compute costs and add to list of children\n","                        child_node.cost = current_node.cost + self.compute_edge_cost(child_candidate)\n","                        all_children.append(child_node)\n","                        if len(all_children) >= self.max_num_children and not in_enumeration:\n","                            return all_children\n","                    # once we've copied it, set current back to the original current\n","                    current.submodules[submod] = orig_fclass\n","                    if not in_enumeration:\n","                        return all_children\n","                else:\n","                    #add submodules\n","                    queue.append(functionclass)\n","        return all_children"],"metadata":{"id":"371EjB0v_AxN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Search Algorithm"],"metadata":{"id":"C6GrKr8U-y6X"}},{"cell_type":"code","source":["class ProgramLearningAlgorithm(object):\n","    \n","    def __init__(self, **kwargs):\n","        pass\n","\n","    def run(self, **kwargs):\n","        raise NotImplementedError\n","\n","\n","class ProgramNodeFrontier(object):\n","    \n","    def __init__(self, capacity=float('inf')):\n","        self.capacity = capacity\n","        self.prioqueue = []\n","\n","    def __len__(self):\n","        return len(self.prioqueue)\n","\n","    def add(self, item):\n","        assert len(item) == 3\n","        assert isinstance(item[2], ProgramNode)\n","        self.prioqueue.append(item)\n","        if len(self.prioqueue) > self.capacity:\n","            # self.sort(tup_idx=0)\n","            popped_f_score, _, popped = self.pop(-1)\n","            log_and_print(\"POP {} with fscore {:.4f}\".format(print_program(popped.program, ignore_constants=True), popped_f_score))\n","\n","    def peek(self, idx=0):\n","        if len(self.prioqueue) == 0:\n","            return None\n","        return self.prioqueue[idx]\n","\n","    def pop(self, idx, sort_fscores=True):\n","        \"\"\"Pops the first item off the queue.\"\"\"\n","        if len(self.prioqueue) == 0:\n","            return None\n","        if sort_fscores:\n","            self.sort(tup_idx=0)\n","        return self.prioqueue.pop(idx)\n","\n","    def sort(self, tup_idx=0):\n","        self.prioqueue.sort(key=lambda x: x[tup_idx])"],"metadata":{"id":"HzmCK7gi-yBK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ASTAR"],"metadata":{"id":"j4RXMm3ZLDsL"}},{"cell_type":"code","source":["class ASTAR_NEAR(ProgramLearningAlgorithm):\n","\n","    def __init__(self, frontier_capacity=float('inf')):\n","        self.frontier_capacity = frontier_capacity\n","\n","    def run(self, graph, trainset, validset, train_config, device, verbose=False):\n","        assert isinstance(graph, ProgramGraph)\n","\n","        log_and_print(\"Training root program ...\")\n","        current = copy.deepcopy(graph.root_node)\n","        initial_score = execute_and_train(current.program, validset, trainset, train_config, \n","            graph.output_type, graph.output_size, neural=True, device=device)\n","        log_and_print(\"Initial training complete. F1-Score from program is {:.4f} \\n\".format(1 - initial_score))\n","        \n","        order = 0\n","        frontier = ProgramNodeFrontier(capacity=self.frontier_capacity)\n","        frontier.add((float('inf'), order, current))\n","        num_children_trained = 0\n","        start_time = time.time()\n","\n","        best_program = None\n","        best_total_cost = float('inf')\n","        best_programs_list = []\n","\n","        while len(frontier) != 0:\n","            current_f_score, _, current = frontier.pop(0)\n","            log_and_print(\"CURRENT program has depth {}, fscore {:.4f}: {}\".format(\n","                current.depth, current_f_score, print_program(current.program, ignore_constants=(not verbose))))\n","            log(\"Creating children for current node/program\")\n","            children_nodes = graph.get_all_children(current)\n","            # prune if more than self.max_num_children\n","            if len(children_nodes) > graph.max_num_children:\n","                children_nodes = random.sample(children_nodes, k=graph.max_num_children)  # sample without replacement\n","            log(\"{} total children to train for current node\".format(len(children_nodes)))\n","\n","            for child_node in children_nodes:\n","                child_start_time = time.time()\n","                log_and_print(\"Training child program: {}\".format(print_program(child_node.program, ignore_constants=(not verbose))))\n","                is_neural = not graph.is_fully_symbolic(child_node.program)\n","                child_node.score = execute_and_train(child_node.program, validset, trainset, train_config, \n","                    graph.output_type, graph.output_size, neural=is_neural, device=device)\n","                log(\"Time to train child {:.3f}\".format(time.time() - child_start_time))\n","                num_children_trained += 1\n","                log(\"{} total children trained\".format(num_children_trained))\n","                child_node.parent = current\n","                child_node.children = []\n","                order -= 1\n","                child_node.order = order  # insert order of exploration as tiebreaker for equivalent f-scores\n","                current.children.append(child_node)\n","\n","                # computing path costs (f_scores)\n","                child_f_score = child_node.cost + child_node.score # cost + heuristic\n","                log_and_print(\"DEBUG: f-score {}\".format(child_f_score))\n","\n","                if not is_neural and child_f_score < best_total_cost:\n","                    best_program = copy.deepcopy(child_node.program)\n","                    best_total_cost = child_f_score\n","                    best_programs_list.append({\n","                            \"program\" : best_program,\n","                            \"struct_cost\" : child_node.cost, \n","                            \"score\" : child_node.score,\n","                            \"path_cost\" : child_f_score,\n","                            \"time\" : time.time()-start_time\n","                        })\n","                    log_and_print(\"New BEST program found:\")\n","                    print_program_dict(best_programs_list[-1])\n","\n","                if is_neural:\n","                    assert child_node.depth < graph.max_depth\n","                    child_tuple = (child_f_score, order, child_node)\n","                    frontier.add(child_tuple)\n","\n","            # clean up frontier\n","            frontier.sort(tup_idx=0)\n","            while len(frontier) > 0 and frontier.peek(-1)[0] > best_total_cost:\n","                frontier.pop(-1)\n","            log_and_print(\"Frontier length is: {}\".format(len(frontier)))\n","            log_and_print(\"Total time elapsed is {:.3f}\\n\".format(time.time()-start_time))\n","\n","        if best_program is None:\n","            log_and_print(\"ERROR: no program found\")\n","\n","        return best_programs_list"],"metadata":{"id":"XulXb2oqHI1u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IDDFS"],"metadata":{"id":"YEArzphALdic"}},{"cell_type":"code","source":["class IDDFS_NEAR(ProgramLearningAlgorithm):\n","\n","    def __init__(self, frontier_capacity=float('inf'), initial_depth=1, performance_multiplier=1.0, depth_bias=1.0, exponent_bias=False):\n","        self.frontier_capacity = frontier_capacity\n","        self.initial_depth = initial_depth\n","        self.performance_multiplier = performance_multiplier # < 1.0 prunes more aggressively\n","        self.depth_bias =depth_bias # < 1.0 prunes more aggressively\n","        self.exponent_bias = exponent_bias # flag to determine it depth_bias should be exponentiated or not\n","\n","    def run(self, graph, trainset, validset, train_config, device, verbose=False, trainset_neural=None):\n","        assert isinstance(graph, ProgramGraph)\n","\n","        log_and_print(\"Training root program ...\")\n","        current = copy.deepcopy(graph.root_node)\n","        dataset = trainset_neural if trainset_neural is not None else trainset\n","        initial_score = execute_and_train(current.program, validset, dataset, train_config,\n","            graph.output_type, graph.output_size, neural=True, device=device)\n","        log_and_print(\"Initial training complete. Score from program is {:.4f} \\n\".format(1 - initial_score))\n","        \n","        # Branch-and-bound search with iterative deepening\n","        current_depth = self.initial_depth\n","        current_f_score = float('inf')\n","        order = 0\n","        frontier = ProgramNodeFrontier(capacity=self.frontier_capacity)\n","        next_frontier = ProgramNodeFrontier(capacity=self.frontier_capacity)\n","        num_children_trained = 0\n","        start_time = time.time()\n","\n","        best_program = None\n","        best_total_cost = float('inf')\n","        best_programs_list = []\n","\n","        log(\"Starting iterative deepening with depth {}\\n\".format(current_depth))\n","        while current_depth <= graph.max_depth:\n","            log_and_print(\"CURRENT program has depth {}, fscore {:.4f}: {}\".format(\n","                current.depth, current_f_score, print_program(current.program, ignore_constants=(not verbose))))\n","            log(\"Creating children for current node/program\")\n","            log(\"Total time elapsed is {:.3f}\".format(time.time()-start_time))\n","            children_nodes = graph.get_all_children(current)\n","\n","            # prune if more than self.max_num_children\n","            if len(children_nodes) > graph.max_num_children:\n","                log(\"Sampling {}/{} children\".format(graph.max_num_children, len(children_nodes)))\n","                children_nodes = random.sample(children_nodes, k=graph.max_num_children) # sample without replacement\n","            log(\"{} total children to train for current node\".format(len(children_nodes)))\n","            \n","            child_tuples = []\n","            for child_node in children_nodes:\n","                child_start_time = time.time()\n","                log(\"Training child program: {}\".format(print_program(child_node.program, ignore_constants=(not verbose))))\n","                is_neural = not graph.is_fully_symbolic(child_node.program)\n","                dataset = trainset_neural if (is_neural and trainset_neural is not None) else trainset\n","                child_node.score = execute_and_train(child_node.program, validset, dataset, train_config,\n","                    graph.output_type, graph.output_size, neural=is_neural, device=device)\n","                log(\"Time to train child {:.3f}\".format(time.time()-child_start_time))\n","                num_children_trained += 1\n","                log(\"{} total children trained\".format(num_children_trained))\n","                child_node.parent = current\n","                child_node.children = []\n","                order -= 1\n","                child_node.order = order # insert order of exploration as tiebreaker for equivalent f-scores\n","\n","                # computing path costs (f_scores)\n","                child_f_score = child_node.cost + child_node.score # cost + heuristic\n","                log(\"DEBUG: f-score {}\".format(child_f_score))\n","                current.children.append(child_node)\n","                child_tuples.append((child_f_score, order, child_node))\n","\n","                if not is_neural and child_f_score < best_total_cost:\n","                    best_program = copy.deepcopy(child_node.program)\n","                    best_total_cost = child_f_score\n","                    best_programs_list.append({\n","                            \"program\" : best_program,\n","                            \"struct_cost\" : child_node.cost, \n","                            \"score\" : child_node.score,\n","                            \"path_cost\" : child_f_score,\n","                            \"time\" : time.time()-start_time\n","                        })\n","                    log_and_print(\"New BEST program found:\")\n","                    print_program_dict(best_programs_list[-1])\n","\n","            # find next current among children, from best to worst\n","            nextfound = False\n","            child_tuples.sort(key=lambda x: x[0])\n","            for child_tuple in child_tuples:\n","                child = child_tuple[2]\n","                if graph.is_fully_symbolic(child.program):\n","                    continue # don't want to expand symbolic programs (no children)\n","                elif child.depth >= current_depth:\n","                    next_frontier.add(child_tuple)\n","                else:\n","                    if not nextfound:\n","                        nextfound = True # first child program that's not symbolic and within current_depth\n","                        current_f_score, current_order, current = child_tuple\n","                        log_and_print(\"Found program among children: {} with f_score {}\".format(\n","                            print_program(current.program, ignore_constants=(not verbose)), current_f_score))\n","                    else:\n","                        frontier.add(child_tuple) # put the rest onto frontier\n","\n","            # find next node in frontier\n","            if not nextfound:\n","                frontier.sort(tup_idx=1) # DFS order\n","                log_and_print(\"Frontier length is: {}\".format(len(frontier)))\n","                original_depth = current.depth\n","                while len(frontier) > 0 and not nextfound:\n","                    current_f_score, current_order, current = frontier.pop(0, sort_fscores=False) # DFS order\n","                    if current_f_score < self.bound_modify(best_total_cost, original_depth, current.depth):\n","                        nextfound = True\n","                        log_and_print(\"Found program in frontier: {} with f_score {}\".format(\n","                            print_program(current.program, ignore_constants=(not verbose)), current_f_score))\n","                    else:\n","                        log_and_print(\"PRUNE from frontier: {} with f_score {}\".format(\n","                            print_program(current.program, ignore_constants=(not verbose)), current_f_score))\n","                log_and_print(\"Frontier length is now {}\".format(len(frontier)))\n","\n","            # frontier is empty, go to next stage of iterative deepening\n","            if not nextfound:\n","                assert len(frontier) == 0\n","                log_and_print(\"Empty frontier, moving to next depth level\")\n","                log_and_print(\"DEBUG: time since start is {:.3f}\\n\".format(time.time() - start_time))\n","\n","                current_depth += 1\n","\n","                if current_depth > graph.max_depth:\n","                    log_and_print(\"Max depth {} reached. Exiting.\\n\".format(graph.max_depth))\n","                    break\n","                elif len(next_frontier) == 0:\n","                    log_and_print(\"Next frontier is empty. Exiting.\\n\")\n","                    break\n","                else:\n","                    log_and_print(\"Starting iterative deepening with depth {}\\n\".format(current_depth))\n","                    frontier = copy.deepcopy(next_frontier)\n","                    next_frontier = ProgramNodeFrontier(capacity=self.frontier_capacity)\n","                    current_f_score, current_order, current = frontier.pop(0)\n","\n","        if best_program is None:\n","            log_and_print(\"ERROR: no program found\")\n","\n","        return best_programs_list\n","\n","    def bound_modify(self, upperbound, current_depth, node_depth):\n","        if not self.exponent_bias:\n","            depth_multiplier = self.performance_multiplier * (self.depth_bias**(current_depth-node_depth))\n","        else:\n","            depth_multiplier = self.performance_multiplier ** (self.depth_bias**(current_depth-node_depth))\n","        return upperbound * depth_multiplier"],"metadata":{"id":"tdLhNoQrHFZK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Enumeration"],"metadata":{"id":"pVVrpHU7VszY"}},{"cell_type":"code","source":["class ENUMERATION(ProgramLearningAlgorithm):\n","\n","    def __init__(self, min_enum_depth=1, max_num_programs=100):\n","        self.min_enum_depth = min_enum_depth\n","        self.max_num_programs = max_num_programs\n","\n","    def run(self, graph, trainset, validset, train_config, device, verbose=False, neur_trainset=None,\n","            neur_validset=None):\n","        assert isinstance(graph, ProgramGraph)\n","\n","        symbolic_programs = []\n","        enum_depth = self.min_enum_depth\n","        while len(symbolic_programs) < self.max_num_programs and enum_depth <= graph.max_depth:\n","            print(\"DEBUG: starting enumerative synthesis with depth {}\".format(enum_depth))\n","            symbolic_programs = self.enumerate2depth(graph, enum_depth)\n","            print(\"DEBUG: {} programs found\".format(len(symbolic_programs)))\n","            enum_depth += 1\n","\n","        log_and_print(\"Symbolic Synthesis: generated {}/{} symbolic programs from candidate program.\".format(\n","            len(symbolic_programs), self.max_num_programs))\n","        \n","        total_eval = min(self.max_num_programs, len(symbolic_programs))\n","        symbolic_programs.sort(key=lambda x: x[\"struct_cost\"])\n","        symbolic_programs = symbolic_programs[:total_eval]\n","\n","        best_total_cost = float('inf')\n","        start_time = time.time()\n","        num_programs_trained = 1\n","        top_programs_list = []\n","        for prog_dict in symbolic_programs:\n","            child_start_time = time.time()\n","            candidate = prog_dict[\"program\"]\n","            log_and_print(\"Training candidate program ({}/{}) {}\".format(\n","                num_programs_trained, total_eval, print_program(candidate, ignore_constants=(not verbose))))\n","            num_programs_trained += 1\n","            score = execute_and_train(candidate, validset, trainset, train_config, \n","                graph.output_type, graph.output_size, neural=False, device=device)\n","            total_cost = score + prog_dict[\"struct_cost\"]\n","            log_and_print(\"Structural cost is {} with structural penalty {}\".format(prog_dict[\"struct_cost\"], graph.penalty))\n","            log_and_print(\"Time to train child {:.3f}\".format(time.time()-child_start_time))\n","            log_and_print(\"Total time elapsed is: {:.3f}\".format(time.time()-start_time))\n","\n","            prog_dict[\"score\"] = score\n","            prog_dict[\"path_cost\"] = total_cost\n","            prog_dict[\"time\"] = time.time() - start_time\n","            top_programs_list.append(prog_dict)\n","            top_programs_list.sort(key=lambda i: i['path_cost'], reverse=True)\n","            top_programs_list = top_programs_list#[-self.num_re_eval:]\n","            if total_cost < best_total_cost:\n","                best_total_cost = total_cost\n","                log_and_print(\"New tentative BEST program found:\")\n","                print_program_dict(top_programs_list[-1])\n","\n","        for program_dict in top_programs_list:\n","            # retrain each program\n","            candidate = program_dict[\"program\"]\n","            program_dict[\"program\"] = copy.deepcopy(candidate)\n","        top_programs_list.sort(key=lambda i: i['path_cost'], reverse=True)\n","        log_and_print(\"Total time elapsed is {:.3f}\".format(time.time() - start_time))\n","        return top_programs_list\n","\n","\n","    def enumerate2depth(self, graph, enumeration_depth):\n","        max_depth_copy = graph.max_depth\n","        graph.max_depth = enumeration_depth\n","        all_programs = []\n","        enumerated = {}\n","        root = copy.deepcopy(graph.root_node)\n","        \n","        def enumerate_helper(program_node):\n","            program_name = print_program(program_node.program, ignore_constants=True)\n","            assert not enumerated.get(program_name)\n","            enumerated[program_name] = True\n","            if graph.is_fully_symbolic(program_node.program):\n","                all_programs.append({\n","                        \"program\" : copy.deepcopy(program_node.program),\n","                        \"struct_cost\" : program_node.cost,\n","                        \"depth\" : program_node.depth\n","                    })\n","            elif program_node.depth < enumeration_depth:\n","                all_children = graph.get_all_children(program_node, in_enumeration=True)\n","                for childnode in all_children:\n","                    if not enumerated.get(print_program(childnode.program, ignore_constants=True)):\n","                        enumerate_helper(childnode)\n","        \n","        enumerate_helper(root)\n","        graph.max_depth = max_depth_copy\n","\n","        return all_programs"],"metadata":{"id":"DhY5ZOyOVxE0"},"execution_count":null,"outputs":[]}]}